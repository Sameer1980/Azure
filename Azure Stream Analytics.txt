Azure Stream Analytics
---------------------------------

Azure Event Hubs is a cloud native data streaming service that can stream millions of events per second, with low latency, from any source to any destination. Event Hubs is compatible with Apache Kafka, and it enables you to run existing Kafka workloads without any code changes.

Azure Event Hubs is the preferred event ingestion layer of any event streaming solution that you build on top of Azure. It seamlessly integrates with data and analytics services inside and outside Azure to build your complete data streaming pipeline to serve following use cases.

Real-time analytics with Azure Stream Analytics to generate real-time insights from streaming data.
Analyze and explore streaming data with Azure Data Explorer.
Create your own cloud native applications, functions, or microservices that run on streaming data from Event Hubs.
Stream events with schema validation using a built-in schema registry to ensure quality and compatibility of streaming data.

Key capabilities
a)Apache Kafka on Azure Event Hubs.
----------------------------------------------
Azure Event Hubs is a multi-protocol event streaming engine that natively supports AMQP, Apache Kafka, and HTTPs protocols. Since it supports Apache Kafka, you bring Kafka workloads to Azure Event Hubs without doing any code change. You don't need to set up, configure, and manage your own Kafka clusters or use a Kafka-as-a-Service offering that's not native to Azure.

Event Hubs is built from the ground up as a cloud native broker engine. Hence, you can run Kafka workloads with better performance, better cost efficiency and with no operational overhead.

b)Schema Registry in Azure Event Hubs
---------------------------------------------------------
Azure Schema Registry in Event Hubs provides a centralized repository for managing schemas of events streaming applications. Azure Schema Registry comes free with every Event Hubs namespace, and it integrates seamlessly with your Kafka applications or Event Hubs SDK based applications.

It ensures data compatibility and consistency across event producers and consumers. Schema Registry enables seamless schema evolution, validation, and governance, and promoting efficient data exchange and interoperability.

Schema Registry seamlessly integrates with your existing Kafka applications and it supports multiple schema formats including Avro and JSON Schemas.

c)Real-time processing of streaming events with Azure Stream Analytics
------------------------------------------------------------------------------------
Event Hubs integrates seamlessly with Azure Stream Analytics to enable real-time stream processing. With the built-in no-code editor, you can effortlessly develop a Stream Analytics job using drag-and-drop functionality, without writing any code.
Alternatively, developers can use the SQL-based Stream Analytics query language to perform real-time stream processing and take advantage of a wide range of functions for analyzing streaming data.

d)Exploring streaming data with Azure Data Explorer
----------------------------------------------------
Azure Data Explorer is a fully managed platform for big data analytics that delivers high performance and allows for the analysis of large volumes of data in near real time. By integrating Event Hubs with Azure Data Explorer, you can easily perform near real-time analytics and exploration of streaming data.

e)Rich ecosystem– Azure functions, SDKs, and Kafka ecosystem
------------------------------------------------------------------
Ingest, buffer, store, and process your stream in real time to get actionable insights. Event Hubs uses a partitioned consumer model, enabling multiple applications to process the stream concurrently and letting you control the speed of processing. Azure Event Hubs also integrates with Azure Functions for serverless architectures.

With a broad ecosystem available for the industry-standard AMQP 1.0 protocol and SDKs available in various languages: .NET, Java, Python, JavaScript, you can easily start processing your streams from Event Hubs. All supported client languages provide low-level integration.

The ecosystem also provides you with seamless integration Azure Functions, Azure Spring Apps, Kafka Connectors, and other data analytics platforms and technologies such as Apache Spark and Apache Flink.

f)Flexible and cost-efficient event streaming
-----------------------------------------------------------------------
You can experience flexible and cost-efficient event streaming through Event Hubs' diverse selection of tiers – including Standard, Premium, and Dedicated. These options cater to data streaming needs ranging from a few MB/s to several GB/s, allowing you to choose the perfect match for your requirements.

g)Scalable
--------------------------------------------
With Event Hubs, you can start with data streams in megabytes, and grow to gigabytes or terabytes. The Auto inflate feature is one of the many options available to scale the number of throughput units or processing units to meet your usage needs.

h)Capture streaming data for long term retention and batch analytics
---------------------------------------------------
Capture your data in near-real time in an Azure Blob storage or Azure Data Lake Storage for long-term retention or micro-batch processing. You can achieve this behavior on the same stream you use for deriving real-time analytics. Setting up capture of event data is fast.


End-to-end

Azure Event Hubs --> Azure Stream analytics --> Power BI dashboard.

On-demand data stream processing solution.
----------------------------------------------
Azure Event Hub]------------------>ADLS-Gen2----> Azure Stream analytics---> Power BI dashboard
IoT Hub         ] ---------------->

Event producer ----------------------> Event Processor -------------------> Event Consumer
Connected Factory Sensors             Azure Stream analytics                 PowerBI
Azure Event Hubs     


a)The "on-demand" approach for processing streaming data involves persisting all incoming data in a data store, such as Azure Data Lake Storage (ADLS) Gen2. This method allows you to collect streaming data over time and store it as static data. You can then process the static data in batches when convenient or during times when compute costs are lower.    
b)An event processor is responsible for the ingestion and transformation of streaming event data.
c)An event producer, which generates an event data stream.
d) An event processor responsible for the ingestion and transformation of streaming event data.
e)An event consumer that displays or consumes event data and acts on it.
f) Azure Blob storage provides an ingestion point for data streaming in an event processing solution that uses static data as a source.
g) Power BI provides a platform for visualizing and analyzing aggregated data in near-real-time. Azure Stream Analytics can target Power BI as an output destination. Processed data is passed into Power BI to facilitate near-real-time dashboard updates.  

Define an Event Hubs namespace.
-----------------------------------------
There are two main steps when creating and configuring new Azure Event Hubs. The first step is to define the Event Hubs namespace. The second step is to create an Event Hub in that namespace.

Define an Event Hubs namespace
---------------------------------
An Event Hubs namespace is a containing entity for managing one or more Event Hubs. Creating an Event Hubs namespace typically involves the following configuration:

Define namespace-level settings
------------------------------------
Certain settings such as namespace capacity (configured using throughput units), pricing tier, and performance metrics are defined at the namespace level. These settings apply to all the Event Hubs within that namespace. If you don't define these settings, a default value is used: 1 for capacity and Standard for pricing tier.

Keep the following aspects in mind:

You must balance your configuration against your Azure budget expectations.


1. Select a unique name for the namespace. The namespace is accessible through this URL: namespace.servicebus.windows.net

2. Define the following optional properties:

Enable Kafka. This option enables Kafka apps to publish events to the Event Hub.

Make this namespace zone redundant. Zone-redundancy replicates data across separate data centers with their independent power, networking, and cooling infrastructures.

Enable Auto-Inflate and Auto-Inflate Maximum Throughput Units. Auto-Inflate provides an automatic scale-up option by increasing the number of throughput units up to a maximum value. This option is useful to avoid throttling in situations when incoming or outgoing data rates exceed the currently set number of throughput units.

Azure CLI commands to create an Event Hubs namespace
To create a new Event Hubs namespace, use the az eventhubs namespace commands. Here's a brief description of the subcommands you'll use in the exercise.

Command                          Description



create                Create the Event Hubs namespace.



authorization-rule       All Event Hubs within the same Event Hubs namespace share common connection credentials. You'll need these credentials when you configure apps to send and receive messages using the Event Hub. This command returns the connection string for your Event Hubs namespace.



Configure a new Event Hub
After you create the Event Hubs namespace, you can create an Event Hub. When creating a new Event Hub, there are several mandatory parameters.

The following parameters are required to create an Event Hub:

Event Hub name - Event Hub name that is unique within your subscription and:

Is between 1 and 50 characters long.

Contains only letters, numbers, periods, hyphens, and underscores.

Starts and ends with a letter or number.

Partition Count - The number of partitions required in an Event Hub (between 2 and 32). The partition count should be directly related to the expected number of concurrent consumers and can't be changed after the hub has been created. The partition separates the message stream so that consumer or receiver apps only need to read a specific subset of the data stream. If not defined, this value defaults to 4.

Message Retention - The number of days (between 1 and 7) that messages will remain available if the data stream needs to be replayed for any reason. If not defined, this value defaults to 7.

You can also optionally configure an Event Hub to stream data to an Azure Blob storage or Azure Data Lake Store account.

Azure CLI commands to create an Event Hub
To create a new Event Hub with the Azure CLI, you'll run the az eventhubs eventhub command set. Here's a brief description of the subcommands we'll be using.

Command

Description

create

Creates the Event Hub in a specified namespace.

show

Displays the details of your Event Hub.

Summary
To deploy Azure Event Hubs, you must configure an Event Hubs namespace, and then configure the Event Hub itself. In the next unit, you'll go through the detailed configuration steps to create a new namespace and Event Hub.

Create an Event Hubs namespace (through sandbox a/c)
--------------------------------------------------------
Create an Event Hubs namespace
Let's create an Event Hubs namespace using Bash shell supported by Azure Cloud shell.

1. First, set default values for the Azure CLI in Cloud Shell. This will keep you from having to enter these values every time. In particular, let's set the resource group and location. Enter the following command into the Azure CLI, and feel free to replace the location with one close to you.

The free sandbox allows you to create resources in a subset of the Azure global regions. Select a region from this list when you create resources:

westus2

southcentralus

centralus

eastus

westeurope

southeastasia

japaneast

brazilsouth

australiasoutheast

centralindia

1. az configure --defaults group=[sandbox Resource Group] location=westus2

2.Create the Event Hubs namespace running the az eventhubs namespace create command. Use the following parameters.

Parameter

Description

--name (required)

Enter a 6-50 characters-long unique name for your Event Hubs namespace. The name should contain only letters, numbers, and hyphens. It should start with a letter and end with a letter or number.

--resource-group (required)

This will be the pre-created Azure sandbox resource group supplied from the defaults.

--location (optional)

Enter the location of your nearest Azure datacenter, this will use your default.

--sku (optional)

The pricing tier for the namespace [Basic / Standard], defaults to Standard. This determines the connections and consumer thresholds.

3)Set the name into an environment variable so we can reuse it.
NS_NAME=ehubns-$RANDOM

4)You can use the Copy button to copy commands to the clipboard. To paste, right-click on a new line in the Cloud Shell window and select Paste, or use the Shift+Insert keyboard shortcut (⌘+V on macOS).

az eventhubs namespace create --name $NS_NAME

Note
Azure will validate the name you enter, and the CLI returns Bad Request if the name exists or is invalid. Try a different name by changing your environment variable and reissuing the command.

Note
Azure will validate the name you enter, and the CLI returns Bad Request if the name exists or is invalid. Try a different name by changing your environment variable and reissuing the command.

5) Fetch the connection string for your Event Hubs namespace running the following command. You'll need this to configure applications to send and receive messages using your Event Hub.

az eventhubs namespace authorization-rule keys list \
    --name RootManageSharedAccessKey \
    --namespace-name $NS_NAME


This command returns a JSON block with the connection string for your Event Hubs namespace that you'll use later to configure your publisher and consumer applications. Save the value of the following keys for later use.

primaryConnectionString

primaryKey

Create an Event Hub
----------------------------------------
Now let's create your new Event Hub.

1. Sign in to the 
Azure portal
 using the same account you activated the sandbox with.

2. Create a new Event Hub by running the eventhub create command. It needs the following parameters.

Parameter

Description

--name (required)

Enter a name for your Event Hub.

--resource-group (required)

Resource group owner.

--namespace-name (required)

Enter the namespace you created.

Let's define the Event Hub name in an environment variable first using the Cloud Shell.

HUB_NAME=hubname-$RANDOM

az eventhubs eventhub create --name $HUB_NAME --namespace-name $NS_NAME

3. View the details of your Event Hub by running the eventhub show command. It needs the following parameters.

az eventhubs eventhub show --namespace-name $NS_NAME --name $HUB_NAME

View the Event Hub in the Azure portal
-----------------------------------------------------------------------------------------------------
Next, let's see what this looks like in the Azure portal.

1. In the Search bar at the top of portal, enter Event Hubs. The Event Hubs pane appears.

2. Select your namespace to open it.

3. in the left menu pane, under Entities, select Event Hubs.

Your Event Hub appears with a status of Activating, and default values for Message Retention (7) and Partition Count of (4).

Configure applications to send or receive messages through an Event Hub.
---------------------------------------------------------------------------
a) Shared access policy name needs to be configured for an application to send messages to an event hub, so that the application can create connection credentials.
b) Primary Shared Access Key needs to be configured for an application to send messages to an event hub so that the application can create connection credentials.
c) Event Hub namespace name needs to be configured for an application to send messages to an event hub


i)Set the storage account name into a variable. It must be between 3 and 24 characters in length and use numbers and lower-case letters only. It also must be unique within Azure.

In this unit, you'll configure these applications to send or receive messages through your Event Hub. These applications are stored in a GitHub repository.

You'll configure two separate applications; one acts as the message sender (SimpleSend), the other as the message receiver (EventProcessorSample). These are Java applications, which enable you to do everything within the browser. However, the same configuration is needed for any platform, such as .NET.

Create a general-purpose, standard storage account
The Java receiver application, that you'll configure in this unit, stores messages in Azure Blob Storage. Blob Storage requires a storage account.

1. In Cloud Shell, create a storage account (general-purpose V2) running the storage account create command. Remember we set a default resource group and location, so even though those parameters are normally required, we can leave them off.

STORAGE_NAME=storagename$RANDOM

2) Next, run the following command to create the storage account.
storage account create --name $STORAGE_NAME --sku Standard_RAGRS --encryption-service blob

If the storage account creation fails, change your environment variable, and try again.

3) List all the access keys associated with your storage account by running the account keys list command. It takes your account name and the resource group (which is defaulted).
az storage account keys list --account-name $STORAGE_NAME

Access keys associated with your storage account are listed. Copy and save the value of key for future use. You'll need this key to access your storage account.

4) View the connections string for your storage account running the following command.
     az storage account show-connection-string -n $STORAGE_NAME

5)This command returns the connection details for the storage account. Copy and save the value of connectionString. It should look something like.

"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=storage_account_name;AccountKey=VZjXuMeuDqjCkT60xX6L5fmtXixYuY2wiPmsrXwYHIhwo736kSAUAj08XBockRZh7CZwYxuYBPe31hi8XfHlWw=="

6) Create a container called messages in your storage account by running the following command. Use the connectionString you copied in the previous step.

az storage container create --name messages --connection-string "<connection string here>"

7) Clone the Event Hubs GitHub repository
Perform the following steps to clone the Event Hubs GitHub repository with git. You can execute this right in Cloud Shell.

1. The source files for the applications that you'll build in this unit are located in a 
GitHub repository
. Run the following commands to make sure that you are in your home directory in Cloud Shell, and then to clone this repository.

cd ~
git clone https://github.com/Azure/azure-event-hubs.git

The repository is cloned to your home folder.

8) Edit SimpleSend.java
You're going to use the built-in Cloud Shell editor. You'll use the editor to modify the SimpleSend application, and add your Event Hubs namespace, Event Hub name, shared access policy name, and primary key. The main commands appear at the bottom of the editor window.

You'll need to write out your edits by pressing Ctrl+O, and then pressing Enter to confirm the output file name. Exit the editor by pressing Ctrl+X. Alternatively, the editor has a "..." menu in the top/right corner for all the editing commands.

1. Change to the SimpleSend folder.

cd ~/azure-event-hubs/samples/Java/Basic/SimpleSend/src/main/java/com/microsoft/azure/eventhubs/samples/SimpleSend

Open Cloud Shell editor in the current folder. This shows a list of files on the left and an editor space on the right.

9) Open the SimpleSend.java file by selecting it from the file list.

10) In the editor, locate and replace the following strings:

"Your Event Hubs namespace name" with the name of your Event Hub namespace.

"Your Event Hub" with the name of your Event Hub.

"Your policy name" with RootManageSharedAccessKey.

"Your primary SAS key" with the value of the primaryKey key for your Event Hub namespace that you saved earlier.

Unlike the terminal window, the editor can use typical copy/paste keyboard accelerator keys for your OS.

If you've forgotten some of them, you can switch down to the terminal window below the editor and run the echo command to list out one of the environment variables. For example:
echo $NS_NAME
echo $HUB_NAME
echo $STORAGE_NAME

11)When you create an Event Hubs namespace, a 256-bit SAS key called RootManageSharedAccessKey is created that has an associated pair of primary and secondary keys that grant send, listen, and manage rights to the namespace. In the previous unit, you displayed the key running an Azure CLI command, and you can also find this key by opening the Shared access policies page for your Event Hubs namespace in the Azure portal.

12) Save SimpleSend.java either through the "..." menu, or the accelerator key (Ctrl+S on Windows and Linux, Cmd+S on macOS).

13) Close the editor with the "..." menu, or the accelerator key CTRL+Q.

14) Use Maven to build SimpleSend.java
You'll now build the Java application running mvn commands.

1. Revert to the main SimpleSend folder.
cd ~/azure-event-hubs/samples/Java/Basic/SimpleSend

15)Build the Java SimpleSend application. This ensures that your application uses the connection details for your Event Hub.

mvn clean package -DskipTests

The build process may take several minutes to complete. Ensure that you see the [INFO] BUILD SUCCESS message before continuing.

16) Edit EventProcessorSample.java
You'll now configure a receiver (also known as subscribers or consumers) application to ingest data from your Event Hub.

For the receiver application, two classes are available: EventHubReceiver and EventProcessorHost. EventProcessorHost is built on top of EventHubReceiver, but provides simpler programmatic interface than EventHubReceiver. EventProcessorHost can automatically distribute message partitions across multiple instances of EventProcessorHost using the same storage account.

In this unit, you'll use the EventProcessorHost method. You'll edit the EventProcessorSample application to add your Event Hubs namespace, Event Hub name, shared access policy name and primary key, storage account name, connection string, and container name.

1. Change to the EventProcessorSample folder running the following command.

cd ~/azure-event-hubs/samples/Java/Basic/EventProcessorSample/src/main/java/com/microsoft/azure/eventhubs/samples/eventprocessorsample

 Open Cloud Shell editor.
code .

Select the EventProcessorSample.java file.

 Locate and replace the following strings in the editor:

----ServiceBusNamespaceName---- with the name of your Event Hubs namespace.

----EventHubName---- with the name of your Event Hub.

----SharedAccessSignatureKeyName---- with RootManageSharedAccessKey.

----SharedAccessSignatureKey---- with the value of the primaryKey key for your Event Hubs namespace that you saved earlier.

----AzureStorageConnectionString---- with your storage account connection string that you saved earlier.

----StorageContainerName---- with "messages".

----HostNamePrefix---- with the name of your storage account.

Save EventProcessorSample.java either with the "..." menu, or the accelerator key (Ctrl+S on Windows and Linux, Cmd+S on macOS).

 Close the editor.

17) Use Maven to build EventProcessorSample.java
Change to the main EventProcessorSample folder running the following command.

cd ~/azure-event-hubs/samples/Java/Basic/EventProcessorSample

Build the Java SimpleSend application running the following command. This ensures that your application uses the connection details for your Event Hub.

mvn clean package -DskipTests

The build process may take several minutes to complete. Ensure that you see a [INFO] BUILD SUCCESS message before continuing.

18)Start the sender and receiver apps
1. Run Java application from the command line by running the java command, and specifying a .jar package. Run the following commands to start the SimpleSend application.

java -jar ./target/simplesend-1.0.0-jar-with-dependencies.jar

When you see Send Complete..., press Enter.

Start the EventProcessorSample application running the following command.

cd ~/azure-event-hubs/samples/Java/Basic/EventProcessorSample
java -jar ./target/eventprocessorsample-1.0.0-jar-with-dependencies.jar

When messages stop appearing on the console, press Enter or press CTRL+C to end the program.

Exercise - Evaluate the performance of the deployed Event Hub using the Azure portal
---------------------------------------------------------------------------------------

This module requires a sandbox to complete. The Sandbox gives you access to free resources. Your personal subscription will not be charged. The sandbox may only be used to complete training on Microsoft Learn. Use for any other reason is prohibited, and may result in permanent loss of access to the sandbox.

In this unit, you'll use the Azure portal to verify your Event Hub is working according to expectations. You'll also test how Event Hub messaging works when it's temporarily unavailable, and use Event Hubs metrics to check the performance of your Event Hub.

View Event Hub activity
----------------------------------------------------------
1. Sign into the 
Azure portal
 using the same account you activated the sandbox with.

2. Find your Event Hub using the Search bar, and open it as we did in the previous exercise.

3. On the Overview page, view the message counts.

4.The SimpleSend and EventProcessorSample applications are configured to send/receive 100 messages. You'll see that the Event Hub has processed 100 messages from the SimpleSend application and has transmitted 100 messages to the EventProcessorSample application.

Test Event Hub resilience
------------------------------------------
Perform the following steps to see what happens when an application sends messages to an Event Hub while it's temporarily unavailable.
1. Resend messages to the Event Hub using the SimpleSend application. Run the following command.
cd ~
cd azure-event-hubs/samples/Java/Basic/SimpleSend
java -jar ./target/simplesend-1.0.0-jar-with-dependencies.jar

2.When you see Send Complete, press Enter.
3.Select your Event Hub in the Overview screen - this will show details specific to the Event Hub. You can also get to this screen with the Event Hubs entry from the namespace page.
4.Select Settings > Properties.
5.Under EVENT HUB STATUS, select Disabled. Save the changes.
Wait for a minimum of five minutes.

6. Select Active under Event Hub state to re-enable your Event Hub, and save your changes.

7. Rerun the EventProcessorSample application to receive messages. Run the following command.
cd ~
cd azure-event-hubs/samples/Java/Basic/EventProcessorSample
java -jar ./target/eventprocessorsample-1.0.0-jar-with-dependencies.jar

When messages stop being appearing on the console, press Enter.

9. Back in the Azure portal, go back to your Event Hub Namespace. If you're still on the Event Hub page, you can use the breadcrumb on the top of the screen to go backwards. Or you can search for the namespace, and select it.

10. Select Monitoring > Metrics.

11.From the Metric list, select Incoming Messages, and then select Add metric.

12. From the Metric list, select Outgoing Messages, and then select Add metric.

13. At the top right of the chart, select Last 24 hours (Automatic) to change the time period to Last 30 minutes to expand the data graph.

You'll see that though the messages were sent before the Event Hub was taken offline for a period, all 100 messages were successfully transmitted.

Important points.
------------------
a) A single publication individual or batch cannot exceed 1MB.
b) An entity that reads data from Event Hubs can be called a consumer.
c) An entity that reads data from Event Hubs can be called a Subscriber.
d) To configure an application to send messages to an Event Hub the Shared Access Policy Name is required.
e) To configure an application to send messages to an Event Hub the Primary shared access key is required.
f) To configure an application to send messages to an Event Hub the Event Hub namespace name is required.
g) To configure an application to send messages to an Event Hub the Event Hub name is required.
h) Publishers can use either HTTPS or AMQP. AMQP opens a socket and can send multiple messages over that socket.



AZURE STREAM ANALYTICS
------------------------------------
Telemetry data/Streaming data } ---- {KafkaConn/ HTTPS/AQMP} > [Event Hub] ---->  {AzureStream Analytics } ----> ( PowerBI)   

Five types of temporal Windowing functions are supported --Tumbling , Hopping , Sliding , Session , Snapshot.

Tumbling windows are simply a hopping window whose hop is equal to its size.
Tumbling Windows (non-overlapping) - [12:01 - 12:10] [12:11 - 12:20] [12:21 - 12:30]

Tumbling window query --

SELECT EventName , Count(*) AS Count from EventStream TIMESTAMP BY EventTimestamp GROUP BY EventName , TumblingWindow(minute,10)

Hopping Window - 3 paremeters --> Time Unit (hrs. , min. sc.) , Window size , Hop size.

Hopping Window query --

-- Count the number of times each event occurs every 10 seconds.

SELECT EventName , Count(*) AS Count from EventStream TIMESTAMP BY EventTimestamp GROUP BY EventName ,  HoppingWindow (second , 10 , 5)

Create a query using tumbling windows
---------------------------------------------------------------------------------
create a Synapse Analytics query using a 
Tumbling Window
. 1) From your Stream Analytics job's blade in the Azure portal , select Query in the left-hand navigation menu.

2)The query will aggregate streaming data received from the Event Hub input and send it to Power BI and Azure Synapse Analytics for visualization and analysis.Clear the edit Query window and paste the following in its place:

WITH Averages AS (
    SELECT
        AVG(engineTemperature) averageEngineTemperature,
        AVG(speed) averageSpeed
    FROM
        eventhub TIMESTAMP BY [timestamp]
    GROUP BY
        TumblingWindow(Duration(second, 2))
),
Anomalies AS (
    select
        t.vin,
        t.[timestamp],
        t.city,
        t.region,
        t.outsideTemperature,
        t.engineTemperature,
        a.averageEngineTemperature,
        t.speed,
        a.averageSpeed,
        t.fuel,
        t.engineoil,
        t.tirepressure,
        t.odometer,
        t.accelerator_pedal_position,
        t.parking_brake_status,
        t.headlamp_status,
        t.brake_pedal_status,
        t.transmission_gear_position,
        t.ignition_status,
        t.windshield_wiper_status,
        t.abs,
        (CASE WHEN a.averageEngineTemperature >= 405 OR a.averageEngineTemperature <= 15 THEN 1 ELSE 0 END) AS enginetempanomaly,
        (CASE WHEN t.engineoil <= 1 THEN 1 ELSE 0 END) AS oilanomaly,
        (CASE WHEN (t.transmission_gear_position = 'first' OR
            t.transmission_gear_position = 'second' OR
            t.transmission_gear_position = 'third') AND
            t.brake_pedal_status = 1 AND
            t.accelerator_pedal_position >= 90 AND
            a.averageSpeed >= 55 THEN 1 ELSE 0 END) AS aggressivedriving
    FROM eventhub t TIMESTAMP BY [timestamp]
    INNER JOIN Averages a ON DATEDIFF(second, t, a) BETWEEN 0 And 2
),
VehicleAverages AS (
    SELECT
        AVG(engineTemperature) averageEngineTemperature,
        AVG(speed) averageSpeed,
        System.TimeStamp() AS snapshot
    FROM
        eventhub TIMESTAMP BY [timestamp]
    GROUP BY
        TumblingWindow(Duration(minute, 2))
)

-- INSERT INTO POWER BI
SELECT
    *
INTO
    powerBIAlerts
FROM
    Anomalies
WHERE aggressivedriving = 1 OR enginetempanomaly = 1 OR oilanomaly = 1

-- INSERT INTO SYNAPSE ANALYTICS
SELECT
    *
INTO
    synapse
FROM
    VehicleAverages


/* 
The query averages the engine temperature and speed over a two-second duration by adding TumblingWindow(Duration(second, 2)) to the query'sGROUP BY clause. Then it selects all telemetry data, including the average values from the previous step, and specifies the following anomalies as new fields:

a. enginetempanomaly: When the average engine temperature is >= 405 or <= 15.

b. oilanomaly: When the engine oil <= 1.

c. aggressivedriving: When the transmission gear position is in first, second, or third, and the brake pedal status is 1, the accelerator pedal position >= 90, and the average speed is >= 55.


The query outputs all fields from the anomalies step into the powerBIAlerts output where aggressivedriving = 1 or enginetempanomaly = 1 or oilanomaly = 1 for reporting. The query also aggregates the average engine temperature and speed of all vehicles over the past two minutes, using TumblingWindow(Duration(minute, 2)), and outputs these fields to the synapse output.

3. Select Save query in the top toolbar when you have finished updating the query.

4. To start the query, select Overview within the Stream Analytics job blade's left-hand navigation menu. On top of the Overview blade, select Start.

5. In the Start job blade that appears, select Now for the job output start time, then select Start. This will start the Stream Analytics job, so it will be ready to start processing and sending your events to Power BI later on.



Exercise – Visualize results in Power BI
-----------------------------------------
In this exercise, you use Power BI  to create a report displaying captured vehicle anomaly data and pin it to a live dashboard for near-real-time updates. Power BI provides a platform for visualizing and analyzing aggregated data in a near-real-time analytics pipeline built on Azure Event Hubs, Azure Stream Analytics, and Power BI.

1. Open a web browser, navigate to https://powerbi.microsoft.com/ , and select Sign in on the upper right.
2. Enter your Power BI credentials you used when creating the Power BI output for Stream Analytics.

3. After signing in, select My Workspace on the left-hand menu.

4. Select the Datasets + dataflows tab on top of the workspace. Locate the dataset named ContosoAutoVehicleAnomalies, then select the Create Report action button to the right of the name. If you do not see the dataset, you may need to wait a few minutes and refresh the page.

Note
It can take several minutes for the dataset to appear. Periodically, you may need to refresh the page before you see the Datasets tab.

5. You should see a new blank report for VehicleAnomalies with the field list on the far right.

6. Select the Map visualization within the Visualizations section on the right.

7.  Drag the city field to Location, and aggressivedriving to Size. Adding these values to the visualization places points of different sizes over cities on the map, depending on how many aggressive driving records there are.

8. Your map should look similar to the following graphic:

9. Select a blank area on the report to deselect the map. Now select the Treemap visualization.

10. Drag the enginetemperature field to Values, then drag the transmission_gear_position field to Group. This will group the engine temperature values by the transmission gear position on the treemap so you can see which gears are associated with the hottest or coolest engine temperatures. The treemap sizes the groups according to the values, with the largest appearing on the upper left and the lowest on the lower right.

11.  Select the down arrow next to the enginetemperature field under Values. Select Average from the menu to aggregate the values by average instead of the sum.

12. Your treemap should look similar to the following graphic:

13. Select a blank area on the report to deselect the treemap. Now select the Area chart visualization.

14. Drag the region field to Legend, the speed field to Values, and the timestamp field to Axis. This will display an area chart with different colors indicating the region and the speed at which drivers travel over time within that region.

15. Select the down arrow next to the speed field under Values. Select Average from the menu to aggregate the values by average instead of the sum.

16. Your area chart should look similar to the following graphic:

17. Select a blank area on the report to deselect the area chart. Now select the Multi-row card visualization.

18.Drag the aggressivedriving field, enginetempanomaly, and oilanomaly fields to Fields.

19. Select the Format tab in the multi-row card settings, then expand Data labels. Set the Text size to 30. Expand Category labels and set the Text size to 12.

20.  Your multi-row card should look similar to the following graphic:

21. Select Save on the upper right of the page.

22. Enter a name, such as "Contoso Auto Vehicle Anomalies," then select Save.

23. Now, let's add this report to a dashboard. Select Pin to a dashboard at the top of the report. You may have to select the ellipses (...) icon to see the option.

24.Select New dashboard, then enter a name, such as "Contoso Auto Vehicle Anomalies Dashboard." Select Pin live. When prompted, select the option to view the dashboard. Otherwise, you can find the dashboard under My Workspace on the left-hand menu.

25. The live dashboard will automatically refresh and update while data is being captured. You can hover over any point on a chart to view information about the item. Select one of the regions in the legend above the average speed chart. All other charts will filter by that region automatically. Select the region again to clear the filter.









